{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Insira seu token de acesso pessoal\n",
    "login(\"Your_HuggingFace_API_Key\",add_to_git_credential=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codigo = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um bot que explica códigos em python\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Me diga o que o seguinte código faz: \\n{codigo}\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "# Melhorando a legibilidade do print\n",
    "response = outputs[0][\"generated_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um bot que recebe explicacões de códigos em python e gera documentações em markdown a partir desses comentários.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{response[2]}\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "# Melhorando a legibilidade do print\n",
    "response_doc = outputs[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre a lista e formatar os itens\n",
    "response_list = response_doc  # Substitua por sua variável exata\n",
    "formatted_response = \"\"\n",
    "\n",
    "for item in response_list:\n",
    "    # Adicionar cada elemento da lista como um bloco formatado\n",
    "    if isinstance(item, dict):\n",
    "        # Formatando dicionário como JSON legível\n",
    "        import json\n",
    "        formatted_response += f\"```json\\n{json.dumps(item, indent=2, ensure_ascii=False)}\\n```\\n\\n\"\n",
    "    else:\n",
    "        # Adicionar elementos simples diretamente\n",
    "        formatted_response += f\"{str(item)}\\n\\n\"\n",
    "\n",
    "# Criar um Markdown-friendly string\n",
    "markdown_response = f\"\"\"\n",
    "# Resposta do Modelo\n",
    "\n",
    "{formatted_response}\n",
    "\"\"\"\n",
    "\n",
    "print(markdown_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo a documentação gerada pelo assistente\n",
    "documentation_content = \"\"\n",
    "\n",
    "for item in response_doc:\n",
    "    if item[\"role\"] == \"assistant\":\n",
    "        documentation_content += item[\"content\"]\n",
    "\n",
    "# Salvando ou adicionando ao arquivo Markdown\n",
    "output_file_path = \"documentacao_codigo_test.md\"\n",
    "\n",
    "# Abrir o arquivo em modo de adição (\"a\") para não sobrescrever o conteúdo existente\n",
    "with open(output_file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "    # Adicionar separadores para distinguir diferentes seções, se necessário\n",
    "    file.write(\"\\n\\n---\\n\\n\")  # Adiciona uma linha horizontal (Markdown: \"---\")\n",
    "    file.write(documentation_content)\n",
    "\n",
    "print(f\"Conteúdo adicionado ao arquivo: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Função para filtrar células de código relevantes\n",
    "def extract_code_cells(notebook_path):\n",
    "    with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    code_cells = []\n",
    "    for cell in nb[\"cells\"]:\n",
    "        if cell[\"cell_type\"] == \"code\":\n",
    "            # Ignorar células vazias ou com apenas comentários\n",
    "            code = cell[\"source\"].strip()\n",
    "            if code and not all(line.strip().startswith(\"#\") for line in code.splitlines()):\n",
    "                code_cells.append(code)\n",
    "    return code_cells\n",
    "\n",
    "# Função para processar cada célula de código com o modelo\n",
    "def process_code_with_model(code_cells, pipe):\n",
    "    markdown_responses = []\n",
    "    for code in tqdm(code_cells, desc=\"Processando células de código\", unit=\"célula\"):\n",
    "        # Alimentar o código para o modelo\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Você é um bot que explica códigos em python\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Me diga o que o seguinte código faz: \\n{code}\"},\n",
    "        ]\n",
    "        outputs = pipe(messages, max_new_tokens=256)\n",
    "        response = outputs[0][\"generated_text\"]\n",
    "\n",
    "        # Gerar explicações em Markdown\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Você é um bot que recebe explicacões de códigos em python e gera documentações em markdown a partir desses comentários.\"},\n",
    "            {\"role\": \"user\", \"content\": response},\n",
    "        ]\n",
    "        outputs = pipe(messages, max_new_tokens=1024)\n",
    "        markdown_response = outputs[0][\"generated_text\"]\n",
    "        markdown_responses.append(markdown_response)\n",
    "    return markdown_responses\n",
    "\n",
    "# Função para salvar em um arquivo Markdown\n",
    "def save_to_markdown_file(markdown_responses, output_file_path):\n",
    "    with open(output_file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        for markdown_response in markdown_responses:\n",
    "            # Extrair apenas o conteúdo do \"role\": \"assistant\"\n",
    "            if isinstance(markdown_response, list):\n",
    "                assistant_responses = [\n",
    "                    msg.get(\"content\", \"\") for msg in markdown_response if msg.get(\"role\") == \"assistant\"\n",
    "                ]\n",
    "                markdown_response = \"\\n\".join(assistant_responses)\n",
    "            elif isinstance(markdown_response, dict):\n",
    "                markdown_response = markdown_response.get(\"content\", \"\") if markdown_response.get(\"role\") == \"assistant\" else \"\"\n",
    "            \n",
    "            # Adiciona separadores e salva no arquivo\n",
    "            file.write(\"\\n\\n---\\n\\n\")  # Separador entre seções\n",
    "            file.write(markdown_response)\n",
    "\n",
    "# Caminho do notebook e arquivo de saída\n",
    "notebook_path = \"channel_segmentation_presal_clean_Full_semi_supervised_CI.ipynb\"  # Substitua pelo caminho do seu notebook\n",
    "output_file_path = \"documentacao_codigo_final.md\"\n",
    "\n",
    "# Extração e processamento\n",
    "code_cells = extract_code_cells(notebook_path)\n",
    "\n",
    "if not code_cells:\n",
    "    print(\"Nenhuma célula de código relevante encontrada.\")\n",
    "else:\n",
    "    print(f\"{len(code_cells)} células de código encontradas para processar.\")\n",
    "    markdown_responses = process_code_with_model(code_cells, pipe)\n",
    "    save_to_markdown_file(markdown_responses, output_file_path)\n",
    "    print(f\"Documentação salva em: {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
