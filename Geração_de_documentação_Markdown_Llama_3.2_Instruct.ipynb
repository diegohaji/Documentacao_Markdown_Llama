{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Insira seu token de acesso pessoal\n",
    "login(\"Your_HuggingFace_API_Key\",add_to_git_credential=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codigo = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um bot que explica códigos em python\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Me diga o que o seguinte código faz: \\n{codigo}\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "# Melhorando a legibilidade do print\n",
    "response = outputs[0][\"generated_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Você é um bot que recebe explicacões de códigos em python e gera documentações em markdown a partir desses comentários.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{response[2]}\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "# Melhorando a legibilidade do print\n",
    "response_doc = outputs[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre a lista e formatar os itens\n",
    "response_list = response_doc  # Substitua por sua variável exata\n",
    "formatted_response = \"\"\n",
    "\n",
    "for item in response_list:\n",
    "    # Adicionar cada elemento da lista como um bloco formatado\n",
    "    if isinstance(item, dict):\n",
    "        # Formatando dicionário como JSON legível\n",
    "        import json\n",
    "        formatted_response += f\"```json\\n{json.dumps(item, indent=2, ensure_ascii=False)}\\n```\\n\\n\"\n",
    "    else:\n",
    "        # Adicionar elementos simples diretamente\n",
    "        formatted_response += f\"{str(item)}\\n\\n\"\n",
    "\n",
    "# Criar um Markdown-friendly string\n",
    "markdown_response = f\"\"\"\n",
    "# Resposta do Modelo\n",
    "\n",
    "{formatted_response}\n",
    "\"\"\"\n",
    "\n",
    "print(markdown_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo a documentação gerada pelo assistente\n",
    "documentation_content = \"\"\n",
    "\n",
    "for item in response_doc:\n",
    "    if item[\"role\"] == \"assistant\":\n",
    "        documentation_content += item[\"content\"]\n",
    "\n",
    "# Salvando ou adicionando ao arquivo Markdown\n",
    "output_file_path = \"documentacao_codigo_test.md\"\n",
    "\n",
    "# Abrir o arquivo em modo de adição (\"a\") para não sobrescrever o conteúdo existente\n",
    "with open(output_file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "    # Adicionar separadores para distinguir diferentes seções, se necessário\n",
    "    file.write(\"\\n\\n---\\n\\n\")  # Adiciona uma linha horizontal (Markdown: \"---\")\n",
    "    file.write(documentation_content)\n",
    "\n",
    "print(f\"Conteúdo adicionado ao arquivo: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393a2753ccc14ffb9e709ea22505e392",
       "version_major": 2,
       "version_minor": 0
      },
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     
    },
    
}
